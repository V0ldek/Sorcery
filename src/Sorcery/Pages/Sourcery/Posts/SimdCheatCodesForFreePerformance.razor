@page "/sourcery/simd-cheat-codes-for-free-performance"
@using Sorcery.Shared.Components.Blogging;
@using Sorcery.Blogging;
@using Sorcery.Shared.Components.Footnotes
@inject BlogBook BlogBook;

@code {
    public static readonly RenderFragment Introduction = __builder =>
    {
        <Paragraph>
            Modern CPUs have special instructions for a more complex mode of execution,
            together with separate registers for those instructions only. These are cheat codes
            that compilers and library developers use to get massive performance gains without
            fundamentally changing the algorithm being executed.
        </Paragraph>
    };
}

<BlogPost Post="BlogBook.SimdCheatCodesForFreePerformance">
    <Paragraph>
        When learning how this whole computer thingy works we usually reason in terms
        of single, simple operations &ndash; set a value, add two values together,
        do a bitwise AND. If you dig into how the CPU executes that stuff you will learn
        about assembly and registers, and that those operations you write in a
        high-level programming language translate to simple instructions that the CPU
        executes &ndash; load a value to a register, add two registers together,
        do a bitwise AND on the register...
    </Paragraph>

    @Introduction

    <Paragraph GutterBottom="false">
        Enter the world of SIMD. This series will introduce the concept from first principles,
        starting with simple concepts at a high-level and then gradually getting us closer to the metal.
        Part 1 starts with good old C# code, without any magic tricks. All the code for this part can be found
        <Link Href="https://github.com/V0ldek/Sorcery/tree/master/src/BlogPostCode/SimdCheatCodesForFreePerformance">on my GitHub</Link>.
    </Paragraph>

    <Header2>What Is SIMD?</Header2>
    <section>
        <Paragraph>
            <strong>SIMD</strong>, <strong>Single Instruction, Multiple Data</strong>, is an umbrella term
            for techniques that allow performing a particular operation on more than one atomic data point.
            For example, while a regular instruction might add two $32$-bit numbers together, a SIMD instruction
            would add <em>multiple individual pairs</em> of $32$-bit numbers together as a single operation.
        </Paragraph>
        <Paragraph>
            The magical world of new instructions and registers I mentioned at the start
            is not actually needed to apply this concept. Consider the following toy problem.
        </Paragraph>
        <MudAlert Variant="Variant.Outlined" Class="mb-4">
            <strong>The Discrepancy Problem</strong> &mdash;
            We are given two streams of $8$-bit measurements from two sensors over some time period.
            We expect them to be the same, but since anomalies can occur we want to detect the first place
            at which they differ, if it exists.
        </MudAlert>
        <Header3>Sequential solution</Header3>
        <section>
            <Paragraph>
                This can be solved with a rather simple sequential loop:
            </Paragraph>
            <CodeBlock LineNumbers="true" Code="@(@"
int? Sequential(ReadOnlySpan<byte> sensor1, ReadOnlySpan<byte> sensor2)
{
    if (sensor1.Length != sensor2.Length)
    {
        throw new ArgumentException(""Unequal stream lengths"");
    }

    for (var i = 0; i < sensor1.Length; i += 1)
    {
        if (sensor1[i] != sensor2[i])
        {
            return i;
        }
    }

    return null;
}")" />
            <Paragraph>
                I'm using <Link Href="https://learn.microsoft.com/en-us/archive/msdn-magazine/2018/january/csharp-all-about-span-exploring-a-new-net-mainstay">
                <Code>ReadOnlySpan&lt;byte&gt;</Code>
                </Link> to abstract the actual input format. It can be any contiguous sequence of bytes somewhere in memory.
            </Paragraph>
        </section>
        <Header3>Engineer's First SIMD</Header3>
        <section>
            <Paragraph>
                To get how to optimise this we need to go a level of abstraction lower and ponder for a second
                what the compiler does for the above code. Well, it will certainly be a loop, steadily incrementing
                the index by one, at each step loading one byte from each sensor into separate registers
                and doing some sort of a comparison on them. CPUs don't usually operate on an $8$-bit elements, though.
                The phrases "$32$-bit" or "$64$-bit" architecture mean that the CPU is equipped with registers
                with those sizes.
                In fact, when you tell your CPU to perform an $8$-bit addition, it will perform a full $32$/$64$-bit
                addition and just ignore whatever happened in the higher $24$/$56$ bits. That means that
                an operation on registers takes the same amount of time regardless of whether we load a byte or $8$ bytes into it.
                So what we're doing here is telling our CPU that can hold $4$/$8$ bytes of data in each hand
                (of which it has around a dozen, here's an amazing mental picture for you) to grab individual
                bytes and leave $75-87.5\%$ of its capacity unused in every iteration of the loop.
            </Paragraph>
            <Paragraph>
                The core insight should gleam on the horizon by now &ndash; maybe we can redesign
                the loop so that we fully utilise the CPU's registers by feeding it $8$ bytes at a time?
            </Paragraph>
            <MudAlert Variant="Variant.Outlined">
                From here on forward I will assume we use a $64$-bit architecture.
                Note that all insights also apply to $32$-bit, in which case we would want to process
                $4$ bytes at a time. You can also use the $8$-byte code on $32$-bit architectures,
                but note that the performance then may not be greater than that of $4$-byte code,
                since the CPU has to play pretend and emulate $64$-bit operations in $32$-bit registers.
            </MudAlert>
            <Paragraph>
                Well, in our problem we can quite naturally just compare $8$ bytes at a time.
                When we found a block of $8$ bytes that differs between sensors, we can then
                find which particular one is different and return that. There's a small issue that
                comes with block-by-block processing &ndash; the stream has to be divisible into those blocks.
                So, if we get $1,000$ bytes of input and want to process it in blocks of $8$, that's easy,
                it's just $125$ blocks. But if we get $1,007$ bytes, suddenly we have clean $125$ blocks
                and the ugly $7$-byte remainder.
            </Paragraph>
            <Paragraph>
                The usual approach in such cases is to run the block-by-block algorithm on most of the input,
                and then process the remaining part sequentially. It will be small (always smaller than block
                size), so it won't have a significant impact on the overall performance. We can
                write a generic function that will split a stream of bytes into the "clean" part and
                the remainder, which will prove very useful:<Footnote>
                    I couldn't use a tuple as the return, since <Code>ReadOnlySpan&lt;&gt;</Code> is special
                    and can't be a type argument, so I used the old multiple-return-values-via-out-parameters
                    approach.
                </Footnote>
            </Paragraph>
            <CodeBlock LineNumbers="true" Code="@(@"
static void DetachFullBlocks(
    ReadOnlySpan<byte> bytes,
    int blockSize,
    out ReadOnlySpan<byte> fullBlocks,
    out ReadOnlySpan<byte> remainder)
{
    var numberOfFullBlocks = bytes.Length / blockSize;
    var prefixLength = numberOfFullBlocks * blockSize;

    fullBlocks = bytes[..prefixLength];
    remainder = bytes[prefixLength..];
}
")" />
            <Paragraph>
                With that handy tool we can write our solution for $64$-bit blocks.
            </Paragraph>
            <CodeBlock LineNumbers="true" Code="@(@"
private int? Simd64(ReadOnlySpan<byte> sensor1, ReadOnlySpan<byte> sensor2)
{
    if (sensor1.Length != sensor2.Length)
    {
        throw new ArgumentException(""Unequal stream lengths"");
    }

    // 64-bit blocks == 8-byte blocks.
    const int Size = 8;

    // Take the cleanly divisible part and leave the remainders for later.
    DetachFullBlocks(sensor1, Size, out var stream1, out var remainder1);
    DetachFullBlocks(sensor2, Size, out var stream2, out var remainder2);

    // Stride by 8 bytes at a time...
    for (var i = 0; i < stream1.Length; i += Size)
    {
        // ... interpreting each 8-byte block as a single 64-bit number.
        var value1 = BitConverter.ToUInt64(stream1[i..(i + Size)]);
        var value2 = BitConverter.ToUInt64(stream2[i..(i + Size)]);

        if (value1 != value2)
        {
            // There is a difference within the block,
            // find where it is exactly.
            for (var j = i; j < i + Size; j += 1)
            {
                if (stream1[j] != stream2[j])
                {
                    return j;
                }
            }
        }
    }

    // Deal with the reminder by running the Sequential version on it.
    return Sequential(remainder1, remainder2) + stream1.Length;
}
")" />
        </section>
        <Header3>Numbers!</Header3>
        <section>
            <Paragraph>
                Let's compare the performance of these solutions! I've also implemented <Code>Simd32</Code>,
                which is a trivial change of the <Code>Size</Code> constant and <Code>BitConverter</Code>
                calls. I run the benchmark on two streams of $1$ megabyte, with only the final byte
                differing between them, forcing all bytes to be compared.
                <Footnote>
                    For completeness, this is run with .NET 7.0.4 on an AMD Ryzen 9 7950X.
                    Benchmarks are, naturally, ran with Benchmark.NET.
                    If you're not familiar with benchmarking .NET code, I have
                    <Link Href="teaching/csharp/9-performance/1-benchmarking">
                    a whole section on benchmarking in the Performance module of my course
                    </Link>!
                </Footnote>
            </Paragraph>
            <MudSimpleTable Class="my-4" Bordered="true" Striped="true" Style="overflow-x: auto; border: solid 1px;">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Mean</th>
                        <th>Error</th>
                        <th>Ratio</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><Code>Sequential</Code></td>
                        <td>$273.67 \mu s$</td>
                        <td>$1.001 \mu s$</td>
                        <td>$1.00$</td>
                    </tr>
                    <tr>
                        <td><Code>Simd32</Code></td>
                        <td>$136.70 \mu s$</td>
                        <td>$1.371 \mu s$</td>
                        <td>$0.50$</td>
                    </tr>
                    <tr>
                        <td><Code>Simd64</Code></td>
                        <td>$67.62 \mu s$</td>
                        <td>$0.476 \mu s$</td>
                        <td>$0.25$</td>
                    </tr>
                </tbody>
            </MudSimpleTable>
            <Paragraph>
                $4 \times$ speedup basically for free! <Footnote>
                    It might be slightly surprising that we get $2$/$4$ times speedups while increasing
                    the size of operations $4$/$8$ times. Such is reality, though, the packed version generates
                    more complex code and introduces its own overhead. There are many variables at play in high-performance
                    code, including memory latency, and without proper profiling we cannot identify them all.
                </Footnote>
            </Paragraph>
        </section>
    </section>
    <Header2>Clever Bitwise Tricks</Header2>
    <section>
        <Paragraph>
            There's a small wrinkle that doesn't really affect performance in this case &ndash; it happens
            at most once at the end &ndash; but understanding
            how to fix it will go a long way. The sequential part of finding the exact place in a block
            where there's a discrepancy is not elegant and there's a clever little trick we can apply
            to make it nicer.
        </Paragraph>
        <Paragraph>
            I want you to forget for a second (or the duration of this article, really) that
            the <Code>UInt64</Code> in the snippet is a number. That's <em>technically</em>
            true, but it's much more useful to think of it as a vector of $8$ bytes.
            In the end, we're not interested in the number itself &ndash; its decimal representation
            is a mere implementation detail, what matters is the $8$ constituent values.
        </Paragraph>
        <Paragraph>
            Try to come up with a solution that finds which element of an $8$-element vector is different
            <em>without using any loops</em>. Here are the building blocks that I give you &ndash;
            you can perform any bitwise operation  on the vectors you want, and you can quickly access some information about
            the vector, like the number of non-zero elements or the location of first such element.
        </Paragraph>
        <Paragraph>
            I'll give you a second.
        </Paragraph>
        <div>
            <Paragraph>
                We can XOR the two vectors together and then ask where the first non-zero element is.
                That element is our discrepancy. If the result of the XOR is zero, it means that both
                vectors were identical. Thus, the check on the blocks now becomes:
            </Paragraph>
            <CodeBlock LineNumbers="true" LineNumbersStartAt="21" Code="@(@"
var xor = value1 ^ value2;

if (xor != 0)
{
    // This is a different 8 than the Size constant, it's for 8 bits in a byte.
    // For example, 26 trailing zero bits is 26 / 8 = 3 full trailing zero bytes.
    var offset = BitOperations.TrailingZeroCount(xor) / 8;
    return i + offset;
}
    ")" />
            <MudPaper Class="pa-2 mb-2 mt-4" Elevation="4">
                <MudGrid Justify="@Justify.Center">
                    <MudItem xs="12" md="6">
                        <MudPaper Class="mr-3 mb-3 pa-3 d-flex align-center justify-center mud-width-full" Elevation="0">
                            <ThemedImage Alt="diagram visualising the xor trick"
                                         Title="The XOR trick"
                                         Src="img/sorcery/simd-cheat-codes-for-free-performance/discrepancy-xor.svg"
                                         Fluid="true"
                                         Width="420"
                                         Height="280" />
                        </MudPaper>
                    </MudItem>
                    <MudItem xs="12" md="6">
                        <MudPaper Height="100%" Class="d-flex align-center justify-center mud-width-full" Elevation="0">
                            <MudText>
                                Visualisation of the XOR trick for $32$-bit registers.
                                The fourth byte in the block differs between streams,
                                so the result of a XOR is non-zero on the fourth byte.
                                Trailing zeroes are used, and dividing their count by $8$
                                gives us how many full bytes are identical. We conclude that
                                it's the byte of index $3$ (counting from $0$) that differs.
                            </MudText>
                        </MudPaper>
                    </MudItem>
                </MudGrid>
            </MudPaper>
        </div>
    </section>
    <Header2>Wide Vector SIMD</Header2>
    <section>
        <Paragraph>
            On a $64$-bit platform the $8$-byte solution utilises the general-purpose registers fully.
            This wouldn't be much of a blogpost if the story ended there, though &ndash; the idea CPU designers
            had was to make bigger registers so that we can handle more data with a single instruction.
            Such extensions are sometimes called <em>vectorial extensions</em> and give us access to the SIMD world,
            where each value/register is a <Highlight>vector</Highlight> of atomic values and are equipped with
            special "wide" operations.
        </Paragraph>
        <Paragraph>
            This is the moment where implementation
            gets hairy &ndash; those additional registers are provided by <em>platform extensions</em> which are,
            well, platform-specific. There is a whole bunch of them providing registers of different sizes
            ($128$, $256$, $512$ bits), different instruction sets, and different performance characteristics.
            What sizes and operations are available depends on the CPU's microarchitecture, and attempting
            to run code specific to, say, $256$-bit wide x86 SIMD on an ARM chip will result in Bad Times.
            <Footnote>In this case, most likely a SIGILL signal that kills the process.</Footnote>
            Instead of diving deep into the various SSEs, AVXs, and NEONs of the world,
            we will use a layer of abstraction that I will call <Highlight>portable SIMD</Highlight>.
        </Paragraph>
        <Paragraph>
            Different languages have different ways of providing portable SIMD. In C# land these facilities
            live in <Code>System.Runtime.Intrinsics</Code> as
            <Link Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.intrinsics.vector128-1?view=net-7.0"><Code>Vector128</Code></Link>,
            <Link Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.intrinsics.vector256-1?view=net-7.0"><Code>Vector256</Code></Link>,
            and (in .NET 8) <Link Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.intrinsics.vector512-1?view=net-8.0"><Code>Vector512</Code></Link>.
            The main purpose of portable SIMD is to allow the programmer to write code with vector acceleration in mind,
            but leave it to the compiler to figure out what specific extensions the target platform supports
            and what code to generate to get the best performance. This is especially natural for a JIT-compiled language
            like C#, where that's <em>already</em> the jitter's job for other code.
        </Paragraph>
        <Paragraph>
            Think about it, if we had to compile to machine code before running the program we wouldn't be able to anticipate what CPU it runs on.
            We would either have to compile for a specific microarchitecture, and then the binary would be non-portable,
            or compile for <em>all</em> architectures and choose the implementation at runtime, which would result
            in a gigantic binary where a lot of the code would be left unused by a particular machine.
            But in .NET, when we run a program the JIT compiler gets a method's bytecode and is supposed
            to turn it into machine code on the fly; but it knows exactly what CPU to target &ndash;
            the one it's running on! So it can always choose exactly the most efficient instructions available
            to the current CPU. It can even try to salvage incompatible vectorial code; say you run
            a method using <Code>Vector256</Code> on a platform that only supports $128$-bit SIMD
            &ndash; the JIT can then split the instructions into two separate <Code>Vector128</Code>
            operations and still get partial acceleration. As a programmer you don't have to worry
            about any of this platform-specific nonsense &ndash; you write the method using
            <Code>System.Runtime.Intrinsics</Code> portable SIMD and then it Just (in Time) Works™.
        </Paragraph>
        <Paragraph>
            Before we jump into SIMD world, here are some preliminaries.
            We'll call operations on vectors that produce other vectors <Highlight>maps</Highlight>,
            and operations that reduce vectors into primitive types <Highlight>projections</Highlight>.
            There are a few ground rules we have to follow:
            <MudList>
                <MudListItem Icon="@Icons.Material.Filled.ChevronRight">
                    all operations on SIMD vectors are immutable &ndash; they read the inputs and produce a fresh result;
                </MudListItem>
                <MudListItem Icon="@Icons.Material.Filled.ChevronRight">
                    we cannot read the contents of a SIMD vector directly, we first need to project
                    it to a primitive type, like an integer;
                </MudListItem>
                <MudListItem Icon="@Icons.Material.Filled.ChevronRight">
                    projection is expensive &ndash; we get vectorial speedups while keeping within the SIMD world,
                    and pay a cost to exit into general-purpose value world.
                </MudListItem>
            </MudList>
            With that in mind, let's explore the two operations we'll need for our acceleration &ndash;
            <Code>cmpeq</Code> and <Code>movemask</Code>.
        </Paragraph>
        <Header3>SIMD Maps and Projections</Header3>
        <section>
            <Paragraph>
                There are SIMD counterparts for most common binary operations, be it arithmetic,
                bitwise logic, or comparison. In the discrepancy problem we're specifically interested in element-wise equality,
                usually dubbed <Highlight><Code>cmpeq</Code></Highlight>.
                The semantics are: the $i$-th element of the result is all-ones if the $i$-th elements
                of input are equal, and zero otherwise. In pseudocode for a single pair of elements:
                <Code>x == y ? 0xFF : 0x00</Code>;<Footnote>
                    It is common to represent values in SIMD using hex notation &ndash; once you get used to it you can
                    easily figure out which bits are set at a glance. <Code>0xFF</Code> is the all-ones byte value
                    &ndash; <Code>255</Code> in decimal, <Code>0b11111111</Code> in binary. Similarly, <Code>0xFFFF</Code>
                    is an all-ones $16$-bit long mask, <Code>0xFFFFFFFF</Code> $32$-bit, and <Code>0xFFFFFFFFFFFFFFFF</Code>
                    $64$-bit. Two more useful ones are <Code>0x01</Code> for the lowest-bit-set byte (decimal <Code>1</Code>)
                    and <Code>0x80</Code> for the highest-bit-set byte (decimal <Code>128</Code>).
                    This gets really handy when one needs to consider individual nibbles, but that's a story
                    for a later post...
                </Footnote> this is executed simultaneously for all pairs.
            </Paragraph>
            <Paragraph>
                As mentioned before, we can't yet use the result in any meaningful way as its locked
                in the opaque <Code>Vector</Code> value, we need to project it first.
                There is a number of standard projections of vectors into fundamental types.
                For example, one can take the sum of all elements in a vector and obtain a regular integer.
                In our case, we are interested in the very special <Highlight><Code>movemask</Code></Highlight> operation.
                It produces a primitive integer whose each bit corresponds to the state of an element in the input vector &ndash;
                the $i$-th bit of the result is set if the most significant bit of the $i$-th element is set.
                In pseudocode for a single vector element getting projected to a single bit: <Code>x & 0x80 == 0x80 ? 1 : 0</Code>;
                this is executed simultaneously for all elements, so from an $N$-byte vector we get an $N$-bit mask as a result.
            </Paragraph>
            <MudPaper Class="pa-2 mb-2 mt-4" Elevation="4">
                <MudGrid Justify="@Justify.Center">
                    <MudItem xs="12" md="6">
                        <MudPaper Class="mr-3 mb-3 pa-3 d-flex align-center justify-center mud-width-full" Elevation="0">
                            <ThemedImage Alt="diagram visualising the cmpeq into movemask operation"
                                         Title="cmpeq-movemask operation"
                                         Src="img/sorcery/simd-cheat-codes-for-free-performance/discrepancy-cmpeq-movemask.svg"
                                         Fluid="true"
                                         Width="420"
                                         Height="280" />
                        </MudPaper>
                    </MudItem>
                    <MudItem xs="12" md="6">
                        <MudPaper Height="100%" Class="d-flex align-center justify-center mud-width-full" Elevation="0">
                            <section>
                                <MudText>
                                    We can combine the equality check with a <Code>movemask</Code> to obtain a mask
                                    similar to the one we worked on in our $32$/$64$-bit algorithms before.
                                    Equality produces vectors of all-one and all-zero values, which are
                                    then collapsed by <Code>movemask</Code> into singular bits of the result.
                                </MudText>
                                <br />
                                <MudText>
                                    This is a small $32$-bit vector example, just for illustrative purposes.
                                    $4$-bit integers don't exist, they can't hurt you.
                                </MudText>
                            </section>
                        </MudPaper>
                    </MudItem>
                </MudGrid>
            </MudPaper>
        </section>
        <Header3>SIMD in C#</Header3>
        <section>
            <Paragraph>
                How do we implement this in C#?
                First we choose the vector size &ndash; let's start with $128$-bit, code for larger ones will be similar &ndash;
                and the C# vector type &ndash; we'll be using <Code>Vector128&lt;T&gt;</Code>.
                The type parameter of the struct is the type of the values we are interpreting as vectors. In our discrepancy
                problem it's <Code>byte</Code> &ndash; <Code>Vector128&lt;byte&gt;</Code> represents $16$ individual <Code>byte</Code> values.
                As an example, you can imagine a similar problem on streams of $32$-bit <Code>int</Code> values, and then we'd
                be using <Code>Vector128&lt;int&gt;</Code> as vectors of $4$ individual <Code>int</Code>s.
                <Footnote>
                    It is perhaps a useful exercise to realise why these types are <em>not</em> interchangeable.
                    For example, the semantics of the <Code>cmpeq</Code> operation are different for
                    <Code>byte</Code>-sized elements and <Code>int</Code>-sized elements; you should be able
                    to construct an example of two $128$-bit vectors <Code>x</Code> and <Code>y</Code>
                    such that the result of <Code>Vector128.Equals(x, y)</Code> is different depending on whether they are
                    of type <Code>Vector128&lt;byte&gt;</Code> or <Code>Vector128&lt;int&gt;</Code>.
                </Footnote>
                The <Code>cmpeq</Code> operation is exposed as
                <Link Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.intrinsics.vector128.equals?view=net-7.0#system-runtime-intrinsics-vector128-equals-1(system-runtime-intrinsics-vector128((-0))-system-runtime-intrinsics-vector128((-0)))">
                the <Code>Equals</Code> method
                </Link> on the <Code>Vector</Code> types, while <Code>movemask</Code> is given as
                <Link Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.intrinsics.vector128.extractmostsignificantbits?view=net-7.0">
                the <Code>ExtractMostSignificantBits</Code> method
                </Link>.
            </Paragraph>
            <Paragraph>
                The tricky part is getting the vectors loaded. Normally when coding in C# we really don't want to think about
                low-level interactions with memory, but here we're pretty close to the hardware. SIMD can only load
                data to a vector if it exists in a single contiguous slice of memory &ndash; the $16$ bytes we want to load
                have to be next to each other, and we need to give a pointer to the first one. We don't want to deal with
                pointers, pointers are icky and require magical <Code>unsafe</Code> stuff. Fortunately, we can use a
                <Highlight>reference variable</Highlight> to, <em>ekhem</em>, refer to the start of our stream,
                and then move it by the appropriate stride equal to the size of our vector.
            </Paragraph>
            <Header4>Reference Variables</Header4>
            <section>
                <Paragraph>
                    Quick detour if you're not familiar with <Code>ref</Code> in C#.
                    Reference variables, also called <em>ref locals</em>, are local variables one level
                    of indirection higher than regular variables &ndash; instead of a value, they hold
                    a reference to a different storage place with the value. This is a bit like a pointer,
                    only that a C# <Code>ref</Code> has a bunch of guarantees around it for memory safety
                    &ndash; it's impossible to have a <Code>ref</Code> that is invalid, the compiler will stop
                    you before that.
                </Paragraph>
                <Paragraph>
                    For example, this:
                    <CodeBlock Code="@(@"
int x = 0;
ref int y = ref x;

x += 1;
Console.WriteLine(y);
y += 1;
Console.WriteLine(x);
Console.WriteLine(y);
")" />
                    prints <Code>1 1 2 2</Code>,<Footnote>
                        See this in action in <Link Href="https://sharplab.io/#gist:08dd877124917daf18f3b5c5533f7e31">SharpLab</Link>.
                    </Footnote>
                    while any attempts to return a potentially invalid <Code>ref</Code> will fail:
                    <CodeBlock Code="@(@"
// Intentionally does not compile.
ref int Foo()
{
    int x = 0;
    return ref x;
}
    ")" />
                </Paragraph>
            </section>
            <Header4>The Code</Header4>
            <section>
                <Paragraph>
                    Okay, let's actually write and run that.
                    First we will take our <Code>ReadOnlySpan</Code>s and ask a helper class
                    <Footnote>
                        <Link Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.interopservices.memorymarshal?view=net-7.0">
                        <Code>MemoryMarshal</Code>
                        </Link>,
                        which exists specifically to help translating between a <Code>ReadOnlySpan</Code> and its contiguous slice memory representation.
                    </Footnote>
                    to give us a <Code>ref</Code> to the first byte, which can be loaded into a <Code>Vector128&lt;byte&gt;</Code>.
                    We will use the <Code>cmpeq</Code> map and the <Code>movemask</Code> projection to get a bitmask where set bits signify elements
                    that are the same.
                    <Footnote>
                        If you look at the code you can see that <Code>ExtractMostSignificantBits</Code>
                        returns a <Code>uint</Code>, even though the mask is actually only $16$-bit wide,
                        so it logically is a <Code>ushort</Code>. The question arises: <em>why</em>?
                        No, seriously, why? I have no idea. If you find out, let me know in the comments.
                    </Footnote>
                    By looking at trailing ones of the mask we can replicate the same
                    algorithm we used before for our $32$ and $64$-bit approaches &ndash; note that the mask here
                    is flipped, we have bits set for identical elements whereas the previous XOR trick
                    left bits set for differing elements. After the check is done we need to use another helper
                    <Footnote>
                        <Link Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.unsafe.add?view=net-7.0#system-runtime-compilerservices-unsafe-add-1">
                        <Code>Unsafe.Add</Code>
                        </Link>;
                        note that we could also keep track of the offset manually in a <Code>nuint</Code> and pass it to the <Code>LoadUnsafe</Code> function,
                        but I didn't want to introduce the concept of a native int here.
                    </Footnote>
                    to move our <Code>ref</Code>s to the streams forward and loop.
                </Paragraph>
                <CodeBlock LineNumbers="true" Code="@(@"
private int? Simd128Portable(ReadOnlySpan<byte> sensor1, ReadOnlySpan<byte> sensor2)
{
    if (sensor1.Length != sensor2.Length)
    {
        throw new ArgumentException(""Unequal stream lengths"");
    }

    // 128-bit blocks == 16-byte blocks.
    const int Size = 16;

    // Take the cleanly divisible part and leave the reminder for later.
    DetachFullBlocks(sensor1, Size, out var stream1, out var remainder1);
    DetachFullBlocks(sensor2, Size, out var stream2, out var remainder2);

    ref byte sensor1Current = ref MemoryMarshal.GetReference(stream1);
    ref byte sensor2Current = ref MemoryMarshal.GetReference(stream2);

    for (var i = 0; i < stream1.Length; i += Size)
    {
        // SAFETY: This operation is safe if the ref bytes actually contain
        // enough initialised values to fill the vector.
        // DetachFullBlocks is the safeguard here.
        Vector128<byte> vector1 = Vector128.LoadUnsafe(ref sensor1Current);
        Vector128<byte> vector2 = Vector128.LoadUnsafe(ref sensor2Current);

        Vector128<byte> cmpeq = Vector128.Equals(vector1, vector2);
        uint mask = Vector128.ExtractMostSignificantBits(cmpeq);

        // If the two vectors are identical, all 16 bits in the mask are set.
        // We compare against the constant 16-set-bits value.
        if (mask != 0xFFFF)
        {
            // We want to count trailing ones, but there is no TrailingOneCount operation.
            // Equivalently, we count the zeroes in the *negated* mask.
            var offset = BitOperations.TrailingZeroCount(~mask);
            return i + offset;
        }

        // SAFETY: This is the same invariant - there have to be at least
        // Size elements to skip over.
        // DetachFullBlocks guarantees this for the entire loop.
        sensor1Current = ref Unsafe.Add(ref sensor1Current, Size);
        sensor2Current = ref Unsafe.Add(ref sensor2Current, Size);
    }

    return Sequential(remainder1, remainder2) + stream1.Length;
}
")" />
                <Paragraph>
                    Whew. The reference handling is a bit scary with all the "unsafe", but the rest should be
                    understandable with our walkthrough. You can play around with the code from
                    <Link Href="https://github.com/V0ldek/Sorcery/tree/master/src/BlogPostCode/SimdCheatCodesForFreePerformance">my GitHub</Link>
                    or on this <Link Href="https://sharplab.io/#gist:03ef2822ab66d8c680703ea870526dec">SharpLab demo</Link>.
                    So, it's a bit harder to code in this paradigm, but are the performance gains
                    worth it?
                </Paragraph>
                <Paragraph>
                    Judge by yourself. I included a $256$-bit version, which is analogous to the above code
                    &ndash; <Code>Vector128</Code> is replaced with <Code>Vector256</Code>, <Code>Size</Code>
                    is set to $32$, ant the mask is compared against <Code>0xFFFFFFFF</Code>.
                </Paragraph>
                <MudSimpleTable Class="my-4" Bordered="true" Striped="true" Style="overflow-x: auto; border: solid 1px;">
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>Mean</th>
                            <th>Error</th>
                            <th>Ratio</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><Code>Sequential</Code></td>
                            <td>$273.95 \mu s$</td>
                            <td>$0.535 \mu s$</td>
                            <td>$1.00$</td>
                        </tr>
                        <tr>
                            <td><Code>Simd32</Code></td>
                            <td>$136.39 \mu s$</td>
                            <td>$0.345 \mu s$</td>
                            <td>$0.50$</td>
                        </tr>
                        <tr>
                            <td><Code>Simd64</Code></td>
                            <td>$69.03 \mu s$</td>
                            <td>$0.130 \mu s$</td>
                            <td>$0.25$</td>
                        </tr>
                        <tr>
                            <td><Code>Simd128Portable</Code></td>
                            <td>$27.34 \mu s$</td>
                            <td>$0.032 \mu s$</td>
                            <td>$0.10$</td>
                        </tr>
                        <tr>
                            <td><Code>Simd256Portable</Code></td>
                            <td>$15.69 \mu s$</td>
                            <td>$0.028 \mu s$</td>
                            <td>$0.06$</td>
                        </tr>
                    </tbody>
                </MudSimpleTable>
                <Paragraph>
                    Changing nothing about the fundamental algorithm we made it <Highlight>$16\times$ faster</Highlight>
                    just by switching to SIMD! And apart from a few relatively uncommon ref-offset operations that's
                    just plain C# code, the JIT does the heavy lifting for us.
                </Paragraph>
                <Paragraph>
                    The question remains, though &ndash; what exactly is it lifting and why is it so heavy?
                </Paragraph>
            </section>
        </section>
        <MudAlert Variant="Variant.Outlined" Class="mb-4">
            We will discuss juicy details in the next part. To not miss it, subscribe to the <Link Href="https://v0ldek.com/feed.rss">RSS feed of Sourcery</Link>!
        </MudAlert>
    </section>
</BlogPost>
