@page "/sourcery/simd-cheat-codes-for-free-performance"
@using Sorcery.Shared.Components.Blogging;
@using Sorcery.Blogging;
@using Sorcery.Shared.Components.Footnotes
@inject BlogBook BlogBook;

@code {
    public static readonly RenderFragment Introduction = __builder =>
    {
        <Paragraph>
            However, modern CPUs have special instructions for a more complex mode of execution,
            together with separate registers for those instructions only. These are cheat codes
            that compilers and library developers use to get massive performance gains without
            fundamentally changing the algorithm being executed.
        </Paragraph>
    };
}

<BlogPost Post="BlogBook.SimdCheatCodesForFreePerformance">
    <Paragraph>
        When learning how this whole computer thingy works we usually reason in terms
        of single, simple operations &ndash; set a value, add two values together,
        do a bitwise AND. If you dig into how the CPU executes that stuff you will learn
        about assembly and registers, and that those operations you write in a
        high-level programming language translate to simple instructions that the CPU
        executes &ndash; load a value to a register, add two registers together,
        do a bitwise AND on the register...
    </Paragraph>
    
    @Introduction

    <Paragraph GutterBottom="false">
        Enter the world of SIMD. This series will introduce the concept from first principles,
        starting with simple concepts at a high-level and then gradually getting us closer to the metal.
        Part 1 starts with good old C# code, without any magic tricks. All the code for this part can be found
        <MudLink Href="https://github.com/V0ldek/Sorcery/tree/master/src/BlogPostCode/SimdCheatCodesForFreePerformance">on my GitHub</MudLink>.
    </Paragraph>

    <Header2>What Is SIMD?</Header2>
    <section>
        <Paragraph>
            <strong>SIMD</strong>, <strong>Single Instruction, Multiple Data</strong>, is an umbrella term
            for techniques that allow performing a particular operation on more than one atomic data point.
            For example, while a regular instruction might add two $32$-bit numbers together, a SIMD instruction
            would add <em>multiple individual pairs</em> of $32$-bit numbers together as a single operation.
        </Paragraph>
        <Paragraph>
            The magical world of new instructions and registers I mentioned at the start 
            is not actually needed to apply this concept. Consider the following toy problem.
        </Paragraph>
        <MudAlert Variant="Variant.Outlined" Class="mb-2">
            <strong>The Discrepancy Problem.</strong> &mdash;
            We are given two streams of $8$-bit measurements from two sensors over some time period.
            We expect them to be the same, but since anomalies can occur we want to detect the first place
            at which they differ, if it exists.
        </MudAlert>
        <Header3>Sequential solution</Header3>
        <section>
            <Paragraph>
                This can be solved with a rather simple sequential loop:
            </Paragraph>
            <CodeBlock LineNumbers="true" Code="@(@"
int? Sequential(ReadOnlySpan<byte> sensor1, ReadOnlySpan<byte> sensor2)
{
    if (sensor1.Length != sensor2.Length)
    {
        throw new ArgumentException(""Unequal stream lengths"");
    }

    for (var i = 0; i < sensor1.Length; i += 1)
    {
        if (sensor1[i] != sensor2[i])
        {
            return i;
        }
    }

    return null;
}")" />
            <Paragraph>
                I'm using <MudLink Href="https://learn.microsoft.com/en-us/archive/msdn-magazine/2018/january/csharp-all-about-span-exploring-a-new-net-mainstay">
                <Code>ReadOnlySpan&lt;byte&gt;</Code>
                </MudLink> to abstract the actual input format. It can be any contiguous sequence of bytes somewhere in memory.
            </Paragraph>
        </section>
        <Header3>Engineer's First SIMD</Header3>
        <section>
            <Paragraph>
                To get how to optimise this we need to go a level of abstraction lower and ponder for a second
                what the compiler does for the above code. Well, it will certainly be a loop, steadily incrementing
                the index by one, at each step loading one byte from each sensor into separate registers
                and doing some sort of a comparison on them. CPUs don't usually operate on an $8$-bit elements, though.
                The phrases "$32$-bit" or "$64$-bit" architecture mean that the CPU is equipped with registers
                with those sizes, in my case $64$-bit.
                In fact, when you tell your CPU to perform an $8$-bit addition, it will perform a full $32$/$64$-bit
                addition and just ignore whatever happened in the higher $24$/$56$ bits. That means that
                an operation on registers takes the same amount of time regardless of whether we load a byte or $8$ bytes into it.
                So what we're doing here is telling our CPU that can hold $8$ bytes of data in each hand
                (this analogy works if you consider the CPU to have around a dozen hands) to grab individual
                bytes and leave $75-87.5\%$ of its capacity unused in every iteration of the loop.
            </Paragraph>
            <Paragraph>
                The core insight should gleam on the horizon by now &ndash; maybe we can redesign
                the loop so that we fully utilise the CPU's registers by feeding it $8$ bytes at a time?
            </Paragraph>
            <MudAlert Variant="Variant.Outlined">
                From here on forward I will assume we use a $64$-bit architecture.
                Note that all insights also apply to $32$-bit, in which case we would want to process
                $4$ bytes at a time. You can also use the $8$-byte code on $32$-bit architectures,
                but note that the performance then may not be greater than that of $4$-byte code,
                since the CPU has to play pretend and emulate $64$-bit operations in $32$-bit registers.
            </MudAlert>
            <Paragraph>
                Well, in our problem we can quite naturally just compare $8$ bytes at a time.
                When we found a block of $8$ bytes that differs between sensors, we can then
                find which particular one is different and return that. There's a small issue that
                comes with block-by-block processing &ndash; the stream has to be divisible into those blocks.
                So, if we get $1,000$ bytes of input and want to process it in blocks of $8$, that's easy,
                it's just $125$ blocks. But if we get $1,007$ bytes, suddenly we have clean $125$ blocks
                and the ugly $7$-byte remainder.
            </Paragraph>
            <Paragraph>
                The usual approach in such cases is to run the block-by-block algorithm on most of the input,
                and then process the remaining part sequentially. It will be small (always smaller than block
                size), so it won't have a significant impact on the overall performance. We can
                write a generic function that will split a stream of bytes into the "clean" part and
                the remainder, which will prove very useful:<Footnote>
                    I couldn't use a tuple as the return, since <Code>ReadOnlySpan&lt;&gt;</Code> is special
                    and can't be a type argument, so I used the old multiple-return-values-via-out-parameters
                    approach.
                </Footnote>
            </Paragraph>
            <CodeBlock LineNumbers="true" Code="@(@"
static void DetachFullBlocks(
    ReadOnlySpan<byte> bytes,
    int blockSize,
    out ReadOnlySpan<byte> fullBlocks,
    out ReadOnlySpan<byte> remainder)
{
    var numberOfFullBlocks = bytes.Length / blockSize;
    var prefixLength = numberOfFullBlocks * blockSize;

    fullBlocks = bytes[..prefixLength];
    remainder = bytes[prefixLength..];
}
")" />
            <Paragraph>
                With that handy tool we can write our solution for $64$-bit blocks.
            </Paragraph>
            <CodeBlock LineNumbers="true" Code="@(@"
private int? Simd64(ReadOnlySpan<byte> sensor1, ReadOnlySpan<byte> sensor2)
{
    if (sensor1.Length != sensor2.Length)
    {
        throw new ArgumentException(""Unequal stream lengths"");
    }
    const int Size = 8;

    // Take the cleanly divisible part and leave the remainders for later.
    DetachFullBlocks(sensor1, Size, out var stream1, out var remainder1);
    DetachFullBlocks(sensor2, Size, out var stream2, out var remainder2);

    // Stride by 8 bytes at a time...
    for (var i = 0; i < stream1.Length; i += Size)
    {
        // ... interpreting each 8-byte block as a single 64-bit number.
        var value1 = BitConverter.ToUInt64(stream1[i..(i + Size)]);
        var value2 = BitConverter.ToUInt64(stream2[i..(i + Size)]);

        if (value1 != value2)
        {
            // There is a difference within the block,
            // find where it is exactly.
            for (var j = i; j < i + Size; j += 1)
            {
                if (stream1[j] != stream2[j])
                {
                    return j;
                }
            }
        }
    }

    // Deal with the reminder by running the Sequential version on it.
    return Sequential(remainder1, remainder2) + stream1.Length;
}
")" />
        </section>
        <Header3>Numbers!</Header3>
        <section>
            <Paragraph>
                Let's compare the performance of these solutions! I've also implemented Simd32,
                which is a trivial change of the <Code>Size</Code> constant and <Code>BitConverter</Code>
                calls. I run the benchmark on two streams of $1$ megabyte, with only the final byte
                differing between them, so that all bytes have to be compared.
                <Footnote>For completeness, this is run with .NET 7.0.4 on an AMD Ryzen 9 7950X.</Footnote>
            </Paragraph>
            <MudSimpleTable Class="my-4" Bordered="true" Striped="true" Style="overflow-x: auto;">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Mean</th>
                        <th>Error</th>
                        <th>Ratio</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Sequential</td>
                        <td>273.67 &mu;s</td>
                        <td>1.001 &mu;s</td>
                        <td>1.00</td>
                    </tr>
                    <tr>
                        <td>Simd32</td>
                        <td>136.70 &mu;s</td>
                        <td>1.371 &mu;s</td>
                        <td>0.50</td>
                    </tr>
                    <tr>
                        <td>Simd64</td>
                        <td>67.62 &mu;s</td>
                        <td>0.476 &mu;s</td>
                        <td>0.25</td>
                    </tr>
                </tbody>
            </MudSimpleTable>
            <Paragraph>
                $4 \times$ speedup basically for free! <Footnote>
It might be slightly surprising that we get $3$/$5$ times speedups while increasing
the size of operations $4$/$8$ times. Such is reality, though, the packed version generates
more complex code and introduces its own overhead. There are many variables at play in high-performance
code, including memory latency, and without proper profiling we cannot identify them all.
</Footnote>
            </Paragraph>
        </section>
    </section>
    <Header2>Clever Bitwise Tricks</Header2>
    <section>
        <Paragraph>
            There's a small wrinkle that doesn't really affect performance, but understanding
            how to fix it will go a long way. The sequential part of finding the exact place in a block
            where there's a discrepancy is not elegant and can be done with a small trick.
        </Paragraph>
        <Paragraph>
            I want you to forget for a second (or the duration of this article, really) that
            the <Code>UInt64</Code> in the snippet is a number. That's <em>technically</em>
            true, but it's much more useful to think of it as a vector of $8$ bytes.
            In the end, we're not interested in the number itself &ndash; its decimal representation
            is a mere implementation detail, what matters is the $8$ constituent values.
        </Paragraph>
        <Paragraph>
            Try to come up with a solution that finds which element of an $8$-element vector is different
            <em>without using any loops</em>. Here are the building blocks that I give you &ndash; 
            you can perform any bitwise operation  on the vectors you want, and you can quickly access some information about
            the vector, like the number of non-zero elements or the location of first such element.
        </Paragraph>
        <div>
            <Paragraph>
                We can XOR the two vectors together and then ask where the first non-zero element is.
                That element is our discrepancy. If the result of the XOR is zero, it means that both
                vectors were identical. Thus, the check on the blocks now becomes:
            </Paragraph>
            <CodeBlock LineNumbers="true" LineNumbersStartAt="20" Code="@(@"
    var xor = value1 ^ value2;

    if (xor != 0)
    {
        // Recall that Size = 4 for 32-bit, 8 for 64-bit.
        var offset = BitOperations.TrailingZeroCount(xor) / Size;
        return i + offset;
    }
    ")" />
            <MudPaper Class="pa-2 mb-1" Elevation="4">
                <MudGrid Justify="@Justify.Center">
                    <MudItem xs="12" md="6">
                        <MudPaper Class="mr-3 mb-3 pa-3 d-flex align-center justify-center mud-width-full" Elevation="0">
                            <ThemedImage Alt="C# sticker" Title="C# sticker" Src="img/sorcery/simd-cheat-codes-for-free-performance/discrepancy-xor.svg" Fluid="true" Width="420" Height="280" />
                        </MudPaper>
                    </MudItem>
                    <MudItem xs="12" md="6">
                        <MudPaper Height="100%" Class="d-flex align-center justify-center mud-width-full" Elevation="0">
                            <MudText>
                                Visualisation of the XOR trick for $32$-bit registers.
                                The fourth byte in the block differs between streams,
                                so the result of a XOR is non-zero on the fourth byte.
                                Trailing zeroes are used, and dividing their count by $8$
                                gives us how many full bytes are identical. We conclude that
                                it's the byte of index $3$ (counting from $0$) that differs.
                            </MudText>
                        </MudPaper>
                    </MudItem>
                </MudGrid>
            </MudPaper>
        </div>
    </section>
    <Header2>Wide Vector SIMD</Header2>
    <section>
        <Paragraph>
            On a $64$-bit platform the $8$-byte solution utilises the general-purpose registers fully.
            The idea behind what we usually call SIMD is rather simple &ndash; let's make bigger registers
            so that we can handle more data with a single instruction. Such extensions are sometimes called
            <em>vectorial extensions</em> and give us access to the SIMD world where each value/register
            is a <Highlight>vector</Highlight> of atomic values.
        </Paragraph>
        <Paragraph>
            This is the moment where implementation
            gets hairy &ndash; those additional registers are provided by <em>platform extensions</em> which are,
            well, platform-specific. There is a whole bunch of them providing registers of different sizes
            ($128$, $256$, $512$ bits), different instruction sets, and different performance characteristics.
            Most modern x86 CPUs have standardised access to most interesting operations, but not all.
            Moreover, ARM SIMD is completely different from x86 SIMD, and there's a separate extension set
            for SIMD on WebAssembly.
            Instead of diving deep into the various SSEs, AVXs, and NEONs of the world,
            we will use a layer of abstraction that I will call <Highlight>portable SIMD</Highlight>.
        </Paragraph>
        <Paragraph>
            Different languages have different ways of providing portable SIMD. In C# land these facilities
            live in <Code>System.Runtime.Intrinsics</Code> as
            <MudLink Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.intrinsics.vector128-1?view=net-7.0"><Code>Vector128</Code></MudLink>,
            <MudLink Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.intrinsics.vector256-1?view=net-7.0"><Code>Vector256</Code></MudLink>,
            and (in .NET 8) <MudLink Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.intrinsics.vector512-1?view=net-8.0"><Code>Vector512</Code></MudLink>.
            The main purpose of portable SIMD is to allow the programmer to write code with vector acceleration in mind,
            but leave it to the compiler to figure out what specific extensions the target platform supports
            and what code to generate to get the best performance. This is especially natural for a JIT-compiled language
            like C#, where that's <em>already</em> the jitter's job for other code.
        </Paragraph>
        <Paragraph>
            Before we jump into SIMD world, here are some preliminaries.
            We'll call operations on vectors that produce other vectors <Highlight>maps</Highlight>,
            and operations that reduce vectors into primitive types <Highlight>projections</Highlight>.
            There are a few ground rules we have to follow:
            <MudList>
                <MudListItem>
                    all operations on SIMD vectors are immutable &ndash; they read the inputs and produce a fresh result;
                </MudListItem>
                <MudListItem>
                    we cannot read the contents of a SIMD vector directly, we first need to project
                    it to a primitive type, like an integer;
                </MudListItem>
                <MudListItem>
                    projection is expensive &ndash; we get vectorial speedups while keeping within the SIMD world,
                    and pay a cost to exit into general-purpose value world.
                </MudListItem>
            </MudList>
        </Paragraph>
        <Header3>SIMD Maps and Projections</Header3>
        <section>
            <Paragraph>
                One of the fundamental binary operation classes
                is comparison &ndash; we're specifically interested in element-wise equality.
                The semantics are: the $i$-th element of the result is all-ones if the $i$-th elements
                of input are equal, and zero otherwise.
            </Paragraph>
            <Paragraph>
                There is a number of standard projections of vectors into fundamental types.
                For example, one can take the sum of all elements in a vector and obtain a regular integer.
                In our case, we are interested in the very special <Highlight>movemask</Highlight> operation.
                It produces a primitive integer whose each bit corresponds to the state of an element in the input vector &ndash;
                The $i$-th bit of the result is set if the most significant bit of the $i$-th element is set.
            </Paragraph>
            <MudPaper Class="pa-2 mb-1" Elevation="4">
                <MudGrid Justify="@Justify.Center">
                    <MudItem xs="12" md="6">
                        <MudPaper Class="mr-3 mb-3 pa-3 d-flex align-center justify-center mud-width-full" Elevation="0">
                            <ThemedImage Alt="C# sticker" Title="C# sticker" Src="img/sorcery/simd-cheat-codes-for-free-performance/discrepancy-cmpeq-movemask.svg" Fluid="true" Width="420" Height="280" />
                        </MudPaper>
                    </MudItem>
                    <MudItem xs="12" md="6">
                        <MudPaper Height="100%" Class="d-flex align-center justify-center mud-width-full" Elevation="0">
                            <section>
                                <MudText>
                                We can combine the equality check with a movemask to obtain a mask
                                similar to the one we worked on in our $32$/$64$-bit algorithms before.
                                Equality produces vectors of all-one and all-zero values, which are
                                then collapsed by movemask into singular bits of the result.
                                </MudText>
                                <br/>
                                <MudText>
                                This is a small $32$-bit example, just for illustrative purposes.
                                $4$-bit integers don't exist, they can't hurt you.
                                </MudText>
                            </section> 
                        </MudPaper>
                    </MudItem>
                </MudGrid>
            </MudPaper>
        </section>
        <Header3>SIMD in C#</Header3>
        <section>
            <Paragraph>
                How do we implement this in C#?
                First we choose the vector size &ndash; let's start with $128$-bit, code for larger ones will be similar &ndash;
                and the C# vector type &ndash; we'll be using <Code>Vector128&lt;T&gt;</Code>.
                The type parameter of the struct is the type of the values we are interpreting as vectors. In our discrepancy
                problem it's <Code>byte</Code> &ndash; <Code>Vector128&lt;byte&gt;</Code> represents $16$ individual <Code>byte</Code> values.
                As an example, you can imagine a similar problem on streams of $32$-bit <Code>int</Code> values, and then we'd
                be using <Code>Vector128&lt;int&gt;</Code> as vectors of $4$ individual <Code>int</Code>s.
            </Paragraph>
            <Paragraph>
                The tricky part is getting the vectors loaded. Normally when coding in C# we really don't want to think about
                low-level interactions with memory, but here we're pretty close to the hardware. SIMD can only load
                data to a vector if it exists in a single contiguous slice of memory &ndash; the $16$ bytes we want to load
                have to be next to each other, and we need to give the pointer to the first one. We don't want to deal with
                pointers, pointers are icky and require magical <Code>unsafe</Code> stuff. Fortunately, we can use a
                <Highlight>reference variable</Highlight> to, <em>ekhem</em>, refer to the start of our stream,
                and then a <Code>nuint</Code> to store the offset within the stream for loading purposes.
            </Paragraph>
            <Header4>Reference Variables</Header4>
            <section>
                <Paragraph>
                    Quick detour if you're not familiar with <Code>ref</Code> in C#.
                    Reference variables, also called <em>ref locals</em>, are local variables one level
                    of indirection higher than regular variables &ndash; instead of a value, they hold
                    a reference to a different storage place with the value. This is a bit like a pointer,
                    only that a C# <Code>ref</Code> has a bunch of guarantees around it for memory safety
                    &ndash; it's impossible to have a <Code>ref</Code> that is invalid, the compiler will stop
                    you before that.
                </Paragraph>
                <Paragraph>
                    For example, this:
                    <CodeBlock Code="@(@"
int x = 0;
ref int y = ref x;

x += 1;
Console.WriteLine(y);
y += 1;
Console.WriteLine(x);
Console.WriteLine(y);
")" />
                    prints <Code>1 1 2 2</Code><Footnote>See <MudLink Typo="@Typo.inherit" Href="https://sharplab.io/#gist:08dd877124917daf18f3b5c5533f7e31">SharpLab gist</MudLink></Footnote>.,
                    while any attempts to return a potentially invalid <Code>ref</Code> will fail:
                    <CodeBlock Code="@(@"
// Intentionally does not compile.
ref int Foo()
{
    int x = 0;
    return ref x;
}
    ")" />
                </Paragraph>
            </section>
            <Header4>The Code</Header4>
            <section>
                <Paragraph>
                    Okay, let's actually write and run that.
                    First we will take our <Code>ReadOnlySpan</Code>s and ask a helper class
                    <Footnote><MudLink Typo="@Typo.inherit" Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.interopservices.memorymarshal?view=net-7.0"><Code>MemoryMarshal</Code></MudLink>,
                    which exists specifically to help translating between a <Code>ReadOnlySpan</Code> and its contiguous slice memory representation.</Footnote>
                    to give us a <Code>ref</Code> to the first byte, which can be loaded into a <Code>Vector128&lt;byte&gt;</Code>.
                    We will use the equality map and movemask projection to get a bitmask where set bits signify elements
                    that are the same. By negating that mask and looking at trailing zeroes we can replicate the same
                    algorithm we used before for our $32$ and $64$-bit approaches. We then need to use another helper
                    <Footnote>
                    <MudLink Typo="@Typo.inherit" Href="https://learn.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.unsafe.add?view=net-7.0#system-runtime-compilerservices-unsafe-add-1"><Code>Unsafe.Add</Code></MudLink>;
                    note that we could also keep track of the offset manually in a <Code>nuint</Code> and pass it to the <Code>LoadUnsafe</Code> function,
                    but I didn't want to introduce the concept of a native int here.
                    </Footnote>
                    to move our <Code>ref</Code>s to the streams forward and loop.
                </Paragraph>
                <CodeBlock LineNumbers="true" Code="@(@"
private int? Simd128Portable(ReadOnlySpan<byte> sensor1, ReadOnlySpan<byte> sensor2)
{
    if (sensor1.Length != sensor2.Length)
    {
        throw new ArgumentException(""Unequal stream lengths"");
    }
    const int Size = 16;

    // Take the cleanly divisible part and leave the reminder for later.
    DetachFullBlocks(sensor1, Size, out var stream1, out var remainder1);
    DetachFullBlocks(sensor2, Size, out var stream2, out var remainder2);

    ref byte sensor1Current = ref MemoryMarshal.GetReference(stream1);
    ref byte sensor2Current = ref MemoryMarshal.GetReference(stream2);

    for (var i = 0; i < stream1.Length; i += Size)
    {
        // SAFETY: This operation is safe if the ref bytes actually contain
        // enough initialised values to fill the vector.
        // DetachFullBlocks is the safeguard here.
        Vector128<byte> vector1 = Vector128.LoadUnsafe(ref sensor1Current);
        Vector128<byte> vector2 = Vector128.LoadUnsafe(ref sensor2Current);

        Vector128<byte> cmpeq = Vector128.Equals(vector1, vector2);
        uint mask = Vector128.ExtractMostSignificantBits(cmpeq);
        
        // Discrepancy occurs only if two elements were different,
        // and one of the bits of the mask is NOT set.
        if (mask != 0xFFFF)
        {
            var offset = BitOperations.TrailingZeroCount(~mask);
            return i + offset;
        }

        // SAFETY: This is the same invariant - there have to be at least
        // Size elements to skip over.
        // DetachFullBlocks guarantees this for the entire loop.
        sensor1Current = ref Unsafe.Add(ref sensor1Current, Size);
        sensor2Current = ref Unsafe.Add(ref sensor2Current, Size);
    }

    return Sequential(remainder1, remainder2) + stream1.Length;
}
")" />
                <Paragraph>
                    Whew. The reference handling is a bit scary with all the "unsafe", but the rest is rather
                    simple. So, it's a bit harder to code in this paradigm, but are the performance gains
                    worth it?
                </Paragraph>
                <Paragraph>
                    Judge by yourself. I included a $256$-bit version, which is analogous to the above code
                    &ndash; <Code>Vector128</Code> is replaced with <Code>Vector256</Code>, <Code>Size</Code>
                    is set to $32$, ant the mask is compared against <Code>0xFFFFFFFF</Code>.
                </Paragraph>
                <MudSimpleTable Class="my-4" Bordered="true" Striped="true" Style="overflow-x: auto;">
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>Mean</th>
                            <th>Error</th>
                            <th>Ratio</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Sequential</td>
                            <td>267.44 &mu;s</td>
                            <td>0.833 &mu;s</td>
                            <td>1.00</td>
                        </tr>
                        <tr>
                            <td>Simd32</td>
                            <td>93.49 &mu;s</td>
                            <td>0.191 &mu;s</td>
                            <td>0.35</td>
                        </tr>
                        <tr>
                            <td>Simd64</td>
                            <td>49.86 &mu;s</td>
                            <td>0.962 &mu;s</td>
                            <td>0.19</td>
                        </tr>
                        <tr>
                            <td>Simd128Portable</td>
                            <td>26.87 &mu;s</td>
                            <td>0.019 &mu;s</td>
                            <td>0.10</td>
                        </tr>
                        <tr>
                            <td>Simd256Portable</td>
                            <td>15.42 &mu;s</td>
                            <td>0.055 &mu;s</td>
                            <td>0.06</td>
                        </tr>
                    </tbody>
                </MudSimpleTable>
                <Paragraph>
                    Changing nothing about the fundamental algorithm we made it <Highlight>$20\times$ faster</Highlight>
                    just by switching to SIMD! And apart from a few relatively uncomplicated ref-offset operations that's
                    just plain C# code, the JIT does the heavy lifting for us.
                </Paragraph>
                <Paragraph>
                    The question remains, though &ndash; what exactly is it lifting and why is it so heavy?
                    Well, that's why this is merely part one.
                </Paragraph>
            </section>
        </section>
        <MudAlert Variant="Variant.Outlined">
            We will discuss juicy details in the next part. To not miss it, subscribe to the <MudLink Href="https://v0ldek.com/feed.rss">RSS feed of Sourcery</MudLink>!
        </MudAlert>
    </section>
</BlogPost>
